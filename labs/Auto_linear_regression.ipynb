{"cells": [{"cell_type": "markdown", "metadata": {"id": "L6chybAVFJW2", "tags": []}, "source": "# **Notebook: Linear regression using gradient descent**\n\nIn this notebook, you will implement a linear regression model by using gradient descent only using `numpy`. In Hand-in assignment 1, you will later extend this code to a full neural network.\n\n**Note:** A linear regression problem can be also solved analytically using the normal equations. However, for the purpose of it being an initial code that you can extend in Hand-in assignment 1, you should solve it here using gradient descent.\n\nWork through the cells below, running each cell in turn. In various places you will see the words \"TO DO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "LdIDglk1FFcG", "tags": []}, "outputs": [], "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd"}, {"cell_type": "markdown", "metadata": {"id": "BGIQwgSJ59W8", "tags": []}, "source": "Load the dataset `'Auto.csv'`. The dataset:  \n\n**Description**:  Gas mileage, horsepower, and other information for 392 vehicles.  \n**Format**: A data frame with 392 observations on the following 9 variables.  \n\n- `mpg`: miles per gallon  \n- `cylinders`: Number of cylinders between 4 and 8\n- `displacement`: Engine displacement (cu. inches)\n- `horsepower`: Engine horsepower\n- `weight`: Vehicle weight (lbs.)\n- `acceleration`: Time to accelerate from 0 to 60 mph (sec.)\n- `year`: Model year (modulo 100)\n- `origin`: Origin of car (1. American, 2. European, 3. Japanese)\n- `name`: Vehicle name  \n*The orginal data contained 408 observations but 16 observations with missing values were removed.*\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "jIfqgJoo6MgD", "tags": []}, "outputs": [], "source": "# The null values are '?' in the dataset. `na_values=\"?\"` recognize the null values.\n# There are null values that will mess up the computation. Easier to drop them by `dropna()`.\n\n# import data\nurl = 'https://uu-sml.github.io/course-sml-public/data/auto.csv'\nauto = pd.read_csv(url, na_values='?', dtype={'ID': str}).dropna().reset_index()"}, {"cell_type": "markdown", "metadata": {"id": "e9ukL0wgHhmT", "tags": []}, "source": "Familiarize yourself with the dataset using `auto.info()`."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "fA8mR7_uHmND", "outputId": "45299945-00a5-45de-b566-993490893407", "tags": []}, "outputs": [], "source": "auto.info()"}, {"cell_type": "markdown", "metadata": {"id": "jK4YhRS0HyK6", "tags": []}, "source": "We will train a linear regression model with `mpg` as output and the remining features as input.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "aAOJo_VBHXWD", "tags": []}, "outputs": [], "source": "# Extract relevant data features\nx_train = auto[['cylinders','displacement','horsepower','weight', 'acceleration','year','origin']].values\ny_train = auto[['mpg']].values"}, {"cell_type": "markdown", "metadata": {"id": "nnUoI0m6GyjC", "tags": []}, "source": "\nNow we are ready to set up the model! The linear regression modell is expressed as\n\\begin{align*}\ny & = f[\\mathbf x, \\boldsymbol \\phi] \\\\\n  & = \\sum_{j=1}^{D_i} \\omega_j x_j +   \\beta \\\\\n  & = \\boldsymbol \\omega^T \\mathbf x + \\beta\n\\end{align*}\nwhere the weights $\\boldsymbol\\omega$ and the bias $\\beta$ are the parameters $\\boldsymbol \\phi =\\{ \\boldsymbol\\omega, \\beta\\}$ of the model.\n\nFirst we initialize all parameters. The weights $\\boldsymbol\\omega$ are stored in  \"weights\" and the bias $\\beta$ is stored in \"bias\". We'll just choose the weights and bias to be equal to zero for now."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "WVM4Tc_jGI0Q", "tags": []}, "outputs": [], "source": "def initialize(D_i):\n  # Arguments\n  # D_i - Input dimension\n\n  # Return\n  # weights, bias -- Weights and bias for the model\n\n  weights = np.zeros((D_i,1))\n  bias = np.zeros((1,1))\n\n  return weights, bias"}, {"cell_type": "markdown", "metadata": {"id": "5irtyxnLJSGX", "tags": []}, "source": "Now let's run our random model! We will evaluate the output for all inputs in one go by vectorizing the code. This reads\n\\begin{align*}\n \\begin{bmatrix}\n  f[\\mathbf x_1,\\boldsymbol\\phi] \\\\\n \\vdots \\\\\n f[\\mathbf x_I,\\boldsymbol\\phi] \\\\\n \\end{bmatrix}\n =\n \\underbrace{\n \\begin{bmatrix}\n\t \\mathbf x_1^T \\\\\n\t \\vdots \\\\\n\t \\mathbf x_{I}^T \\\\\n \\end{bmatrix}\n }_{\\verb|net_input|}\n \\boldsymbol \\omega + \\beta\n \\end{align*}\n\n where $\\beta$ is added to each row (called broadcasting in Python)"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "LgquJUJvJPaN", "tags": []}, "outputs": [], "source": "def forward_pass(net_input, weights, bias):\n  # Arguments\n  # net_input -- Input X for the model\n  # weights, bias -- Weights and bias for the model\n  #\n  # Return\n  # net_output -- Output y of the model\n\n  # TODO -- Replace the lines below\n  net_output = 0  \n\n  return net_output "}, {"cell_type": "markdown", "metadata": {"id": "SxVTKp3IcoBF", "tags": []}, "source": "Now let's define a loss function.  We'll just use the least squares loss function. We'll also write a function to compute dloss_doutput"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "6XqWSYWJdhQR", "tags": []}, "outputs": [], "source": "def least_squares_loss(net_output, y):\n    # Arguments    \n    # net_output -- Output for the model\n    # y -  true label vector, one-hot encoded\n    #\n    # Return\n    # net_output -- Output y of the model\n    \n    I = y.shape[0]\n    cost = np.sum((net_output-y) * (net_output-y))/I\n    return cost\n\ndef d_loss_d_output(net_output, y):\n    # Arguments    \n    # net_output -- Output for the model\n    # y -  true label vector, one-hot encoded\n    #\n    # Return \n    # d_output -- derivative of loss with respect to output of the model    \n    \n    I = y.shape[0]\n    # TODO Calculate the derivatives of the loss with respect to output of the model, see first item of Preparatory exercise 2.2   \n    # REPLACE THIS LINE\n    d_output =  np.zeros_like(bias)\n\n    return d_output "}, {"cell_type": "markdown", "metadata": {"id": "98WmyqFYWA-0", "tags": []}, "source": "Now let's compute the gradient of the loss with respect to the parameters. To do that we need to do both a forward pass and then a backward pass through the model. We have already done the forward pass.  Let's compute the backward pass."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "LJng7WpRPLMz", "tags": []}, "outputs": [], "source": "# Main backward pass routine\ndef backward_pass(weights, bias, net_input, net_output, y):\n  # Arguments\n  # weights, bias -- Weights and bias for the model\n  # net_input, net_output -- Input and output for the model\n  # y -  true label vector, one-hot encoded\n  #\n  # Return\n  # dl_dweights, dl_dbias -  derivative of loss with respect to weights andb bias\n\n  # Compute derivatives of the loss with respect to the network output\n  dl_df = np.array(d_loss_d_output(net_output,y))\n\n  # TODO Calculate the derivatives of the loss with respect to the bias using preparatpry exercise 2.1-2.2\n  # REPLACE THIS LINE\n  dl_dbias = np.zeros_like(bias)\n  # TODO Calculate the derivatives of the loss with respect to the weights using preparatpry exercise 2.1-2.2\n  # REPLACE THIS LINE\n  dl_dweights = np.zeros_like(weights)\n\n  return dl_dweights, dl_dbias "}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "We can check we got this right using a trick known as **finite differences**.  If we evaluate the function and then change one of the parameters by a very small amount and normalize by that amount, we get an approximation to the gradient, so:\n\n\\begin{align}\n\\frac{\\partial L}{\\partial \\beta}&\\approx & \\frac{L[\\beta+\\delta, \\omega_1,\\dots,\\omega_{D_i}]-L[\\beta, \\omega_1,\\dots,\\phi_{D_i}]}{\\delta}\\\\\n\\frac{\\partial L}{\\partial \\omega_{1}}&\\approx & \\frac{L[\\beta, \\omega_1+\\delta,\\dots,\\omega_{D_i}]-L[\\beta, \\omega_1,\\dots,\\omega_{D_i}]}{\\delta}\\\\\n&\\,\\, \\vdots  \\\\\n\\frac{\\partial L}{\\partial \\omega_{D_i}}&\\approx & \\frac{L[\\beta, \\omega_1,\\dots,\\omega_{D_i}+\\delta]-L[\\beta, \\omega_1,\\dots,\\omega_{D_i}]}{\\delta}\n\\end{align}\n\nWe can't do this when there are many parameters;  for a million parameters, we would have to evaluate the loss function one million plus one times, and usually computing the gradients directly is much more efficient.\n\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "# Compute the gradient using your function\n# Sefine a random \nD_i = 5\nI = 10\nnet_input = np.random.normal(size=(I,D_i))\ny = np.random.normal(size=(I,1))\nweights = np.random.normal(size=(D_i,1))\nbias = np.random.normal()\n\n\n# Compute gradient with forward and backward pass\nnet_output = forward_pass(net_input, weights, bias)\ndl_dweights, dl_dbias = backward_pass(weights, bias, net_input, net_output, y)\n\n# Approximate the gradients with finite differences\ndelta = 0.0001\n\n# Start with bias\nbias_delta =  bias + delta \nnet_output_delta = forward_pass(net_input, weights, bias_delta)\ndl_dbias_est = (least_squares_loss(net_output_delta, y)-least_squares_loss(net_output, y))/delta\n\nprint(\"Your derivative of bias\")\nprint(dl_dbias)\n\nprint(\"Approximated derivative of bias\")\nprint(dl_dbias_est)\n\n\n# Now the weights\ndl_dweights_est  = np.zeros_like(dl_dweights)\n\nfor j in range(D_i):\n    weights_delta = np.array(weights)\n    weights_delta[j] += delta\n    net_output_delta = forward_pass(net_input, weights_delta, bias)\n    dl_dweights_est[j] = (least_squares_loss(net_output_delta, y)-least_squares_loss(net_output, y))/delta\n\nprint(\"Your derivative of weights\")\nprint(dl_dweights)\n\nprint(\"Approximated derivative of weights\")\nprint(dl_dweights_est)\n"}, {"cell_type": "markdown", "metadata": {"id": "EKbaO8FUCm7D", "tags": []}, "source": "Once we have checked that the derivatives match we can proceed.\n\nWe also need a function for how to update the parameters."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "PK-UtE3hreAK", "tags": []}, "outputs": [], "source": "def update_parameters(weights, bias, dl_dweights, dl_dbias, alpha):\n  # Arguments\n  # weights, bias -- Weights and bias for the model\n  # dl_dweights, dl_dbias -  derivative of loss with respect to weights and bias\n  # alpha -  learning rate\n  #\n  # Return\n  # weights, bias -- Updated weights and bias for the model\n\n  # TODO -- Replace the lines below    \n  weights = weights\n  bias = bias\n\n  return weights, bias "}, {"cell_type": "markdown", "metadata": {"id": "RUZcv1ikC2sj", "tags": []}, "source": "Here we have the main function for training the model."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "Z0ZKLVooUwJG", "tags": []}, "outputs": [], "source": "def train_model(x_train,y_train,iterations,alpha):\n  # Arguments\n  # x, y -- Input and output for training data\n  # iterations -- number of iterations\n  # alpha -- learning rate\n    \n  D_i = x_train.shape[1]\n  weights, bias = initialize(D_i)\n\n  train_losses, test_losses, train_accuracies, test_accuracies = [], [], [], []\n\n  for iteration in range(iterations):\n\n    # Forward pass\n    net_output = forward_pass(x_train,weights, bias)\n\n    # Backward pass\n    dl_dweights, dl_dbias = backward_pass(weights, bias, x_train, net_output, y_train)\n\n    # Update parameters\n    weights, bias = update_parameters(weights, bias, dl_dweights, dl_dbias, alpha)\n\n    # For every iteration compute loss\n    if iteration % 1 == 0:\n      train_loss = least_squares_loss(net_output, y_train)\n      #print(\"Iteration %i: Train loss: %f.\" %(iteration, train_loss))\n      train_losses.append(train_loss)\n\n  train_losses = np.array(train_losses)\n  return train_losses, weights, bias"}, {"cell_type": "markdown", "metadata": {"id": "JkN_weOCKRu9", "tags": []}, "source": "Function for doing the plotting."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "pDDOTQNRlMOH", "tags": []}, "outputs": [], "source": "from matplotlib import pyplot as plt\nimport numpy as np\n\ndef training_curve_plot(train_losses):\n  \"\"\" convenience function for plotting train loss\n  \"\"\"\n  lg=13\n  md=10\n  sm=9\n  x = range(1, len(train_losses)+1)\n  plt.plot(x, train_losses, label=f'Final train loss: {train_losses[-1]:.4f}')\n  plt.xlabel('Iteration', fontsize=md)\n  plt.ylabel('Loss', fontsize=md)\n  plt.legend(fontsize=sm)\n  plt.tick_params(axis='both', labelsize=sm)\n  plt.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n  plt.show()\n\ndef abline(slope, intercept):\n  \"\"\"Plot a line from slope and intercept\"\"\"\n  axes = plt.gca()\n  x_vals = np.array(axes.get_xlim())\n  y_vals = intercept + slope * x_vals\n  plt.plot(x_vals, y_vals, '--')"}, {"cell_type": "markdown", "metadata": {"id": "n6rEiDnFKVk9", "tags": []}, "source": "For gradient descent to work properly it is important to normalze the data. This can be done, for example, by subtracting the mean and dividing with the standard deviation."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "ydzhRJY3D7Td", "tags": []}, "outputs": [], "source": "# Normalize the input data\nx_train_norm = (x_train - np.mean(x_train,axis=0))/np.std(x_train,axis=0)"}, {"cell_type": "markdown", "metadata": {"id": "xe9E2Kd8LYDD", "tags": []}, "source": "Now we are ready to train the model! Fit **two** linear regression models for the two choices of inputs ((i) only `horsepower` and (ii) all except `name`)."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 448}, "id": "hCMm5ElFirvX", "outputId": "66b7447b-6155-4e7a-ee1a-7d41a8d1b6a1", "tags": []}, "outputs": [], "source": "# Train the model with all features\ntrain_losses, weights, bias = train_model(x_train_norm,y_train,iterations=100,alpha=0.1);\ntraining_curve_plot(train_losses)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 861}, "id": "VThrtLDDL7hb", "outputId": "4d2498b8-5b18-4e04-dc3e-50456b2e7b53", "tags": []}, "outputs": [], "source": "# Train the model with horsepower only as input\ntrain_losses, weights, bias = train_model(x_train_norm[:,[2]],y_train,iterations=100,alpha=0.1);\ntraining_curve_plot(train_losses)\n# Plot the linear regression model together with the data\nplt.plot(x_train_norm[:,2],y_train,'.')\nabline(weights[0,0], bias[0])"}, {"cell_type": "markdown", "metadata": {"id": "VBdamdHag2ic", "tags": []}, "source": "With a correct implementation, the final cost for the two models should be less than 12 and less than 25, respectively.\n\n**TO DO:**  State the final loss for each model."}, {"cell_type": "markdown", "metadata": {"id": "ca31yizDfHKe", "tags": []}, "source": "**TO DO:** To study the relationship between *learning rate* and *number of iterations*, try learning rates `[1, 1e-1, 1e-2, 1e-3, 1e-4]` for the two models. Provide a high level interpretation of what you observe evaluating these learning rates."}, {"cell_type": "markdown", "metadata": {"id": "sj8VWLwtgl6i", "tags": []}, "source": "**TO DO:** Try repeating any of the previous experiments without normalizing the input. What happens then? Can you find a learning rate sch that we converge?"}], "metadata": {"anaconda-cloud": {}, "celltoolbar": "Tags", "colab": {"provenance": []}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.7"}}, "nbformat": 4, "nbformat_minor": 1}