{"cells": [{"cell_type": "markdown", "id": "2e9214ef", "metadata": {"tags": []}, "source": "# Introduction to PyTorch\n\nYou've implemented a lot of neural network functionalities, such as the forward/backward propagation, ReLU activation, Softmax, Mini-batch SGD, etc., using *Numpy*. You've also put significant effort into making your implementations efficient and fully vectorized in Hand-in assignment 1.\n\nIn this notebook, we're going to leave behind your beautiful codebase and instead migrate to a popular and powerful deep learning framework: *PyTorch*. **This make you get started to solve the first part of Hand-in assignment 2.**\n\n### The motivation of using deep learning frameworks\n\n- GPU acceleration: Your code can run on GPUs, enabling much faster training and inference.\n- Automatic differentiation: This simplifies the implementation of complex neural networks and differentiable functions.\n\nYou can find more tutorials in the official website: https://docs.pytorch.org/tutorials\n\n### Table of contents\n\nThis notebook contains two main parts:\n1. **Part I, PyTorch Basics.** You will learn and work with PyTorch tensors.\n2. **Part II, MNIST Classification.** You will implement a neural network using PyTorch for MNIST classification and aiming to get the same or comparable results to those from Assignment-1."}, {"cell_type": "markdown", "id": "e3aa28f5", "metadata": {"tags": []}, "source": "# Part I: PyTorch Basics\n\nNote: Before running the following cells, you can manually switch to a GPU device on Colab by clicking `Runtime -> Change runtime type` and selecting `GPU` under `Hardware Accelerator`."}, {"cell_type": "code", "execution_count": null, "id": "793a4b3d", "metadata": {"tags": []}, "outputs": [], "source": "import math\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\n\nprint(\"PyTorch version: \", torch.__version__)"}, {"cell_type": "markdown", "id": "89e886f5", "metadata": {"tags": []}, "source": "**Numpy array vs. Pytorch tensor**\n\nPyTorch tensors are a specialized data structure that are very similar to NumPy\u2019s ndarrays, except that tensors can run on GPUs and support automatic differentiation. If you\u2019re already familiar with ndarray, you\u2019ll be right at home with the Tensor API. If not, no worries\u2014just follow along!\n\n**Initialize a tensor**"}, {"cell_type": "code", "execution_count": null, "id": "fb709aaa", "metadata": {"tags": []}, "outputs": [], "source": "data = [[1, 2], [3, 4]]\ndata_numpy = np.array(data)\nprint(\"data:\", data)\n\n# directly from the data\ntensor_from_data = torch.tensor(data)\nprint(tensor_from_data)"}, {"cell_type": "code", "execution_count": null, "id": "16344cb6", "metadata": {"tags": []}, "outputs": [], "source": "# from the Numpy ndarrays\ntensor_from_numpy = torch.from_numpy(data_numpy)\nprint(tensor_from_data)"}, {"cell_type": "code", "execution_count": null, "id": "57be4c64", "metadata": {"tags": []}, "outputs": [], "source": "# initialize with random or constant values\nrand_tensor = torch.rand(tensor_from_data.shape)\nprint(rand_tensor)\nzero_tensor = torch.zeros(tensor_from_data.shape)\nprint(zero_tensor)\nones_tensor = torch.ones(tensor_from_data.shape)\nprint(ones_tensor)"}, {"cell_type": "markdown", "id": "30ecf804", "metadata": {"tags": []}, "source": "**Tensor to numpy**"}, {"cell_type": "code", "execution_count": null, "id": "b4dd0cb0", "metadata": {"tags": []}, "outputs": [], "source": "numpy_from_tensor = tensor_from_data.numpy()\nnumpy_from_tensor"}, {"cell_type": "markdown", "id": "552f32b9", "metadata": {"tags": []}, "source": "**Attributes of a Tensor** \n\nTensor attributes describe their shape, datatype, and the device on which they are stored."}, {"cell_type": "code", "execution_count": null, "id": "ac07e10e", "metadata": {"tags": []}, "outputs": [], "source": "tensor1 = tensor_from_numpy.float()\ntensor2 = rand_tensor.float()\ntensor = torch.mul(tensor1, tensor2)\nprint(tensor)\n\nprint(f\"Shape of tensor: {tensor.shape}\")\nprint(f\"Datatype of tensor: {tensor.dtype}\")\nprint(f\"Device tensor is stored on: {tensor.device}\")"}, {"cell_type": "markdown", "id": "b7448981", "metadata": {"tags": []}, "source": "**Tensor manipulation on the GPU deivce**\n\nNote: the following example will be executed on GPU only if you have access to a GPU."}, {"cell_type": "code", "execution_count": null, "id": "3ff4b04e", "metadata": {"tags": []}, "outputs": [], "source": "print(\"Use GPU: \", torch.cuda.is_available())\n\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n    \ntensor1_gpu = tensor1.to(device)\ntensor2_gpu = tensor2.to(device)\ntensor_gpu = torch.mul(tensor1_gpu, tensor2_gpu)\nprint(\"tensor device: \", tensor_gpu.device)"}, {"cell_type": "markdown", "id": "b9550a68", "metadata": {"tags": []}, "source": "You can also manually convert the tensor back to CPU."}, {"cell_type": "code", "execution_count": null, "id": "7e25e3cc", "metadata": {"tags": []}, "outputs": [], "source": "tensor_cpu = tensor_gpu.cpu()\nprint(\"tensor device: \", tensor_cpu.device)"}, {"cell_type": "markdown", "id": "97694b41", "metadata": {"tags": []}, "source": "**Back-propagation with PyTorch's automatic differentiation engine**\n\nRecall that training a neural network typically happens in three steps: *Forward Propagation*, *Backward Propagation*, and *Gradient Descent Optimization*. \n\nDuring the forward pass in our model, PyTorch defines a computational graph, whose nodes are tensors and whose edges are the functions mapping one input tensor to another output tensor. By backpropagating this graph, we obtain the desired gradients which can be used to optimzie the parameters."}, {"cell_type": "code", "execution_count": null, "id": "0f95f9d7", "metadata": {"tags": []}, "outputs": [], "source": "# state that we want to obtain the gradients of x\nx1 = torch.tensor([2., 3.], requires_grad=True)\nx2 = torch.tensor([1., 2.], requires_grad=True)\n\n# run the forward pass and build the computational graph\nz = torch.sum(x1 - x2)\nprint(f\"Output of the forward pass: {z}\\n\")\n\n# backpropagate the graph\nz.backward()\n\nprint(\"First computation\")\nprint(f\"Gradient information of x1: {x1.grad}\")\nprint(f\"Gradient information of x2: {x2.grad}\\n\")\n\n# Repeat the computations and backpropagate again, the gradients will be accumulated.\nz = torch.sum(x1 - x2)\n\n# backpropagate the graph\nz.backward()\n\nprint(\"Second computation\")\nprint(f\"Gradient information of x1: {x1.grad}\")\nprint(f\"Gradient information of x2: {x2.grad}\")"}, {"cell_type": "markdown", "id": "fefc6e18", "metadata": {"tags": []}, "source": "To override the gradients, we have to set them to zero before running the forward pass."}, {"cell_type": "code", "execution_count": null, "id": "0132111c", "metadata": {"tags": []}, "outputs": [], "source": "# set the gradient of x to zero\nx1.grad.zero_()\nx2.grad.zero_()\n\n# run the forward pass and build the computational graph\nz = torch.sum(x1 - x2)\n\n# backpropagate the graph\nz.backward()\n\nprint(f\"Gradient information of x1: {x1.grad}\")\nprint(f\"Gradient information of x2: {x2.grad}\")"}, {"cell_type": "markdown", "id": "c880988e", "metadata": {"tags": []}, "source": "If we want perform operations that should not be tracked by PyTorch, we can wrap them inside a torch.no_grad() clause."}, {"cell_type": "code", "execution_count": null, "id": "474846fa", "metadata": {"tags": []}, "outputs": [], "source": "with torch.no_grad():\n    z = torch.sum(x1 - x2)\nprint(z)"}, {"cell_type": "markdown", "id": "49384465", "metadata": {"tags": []}, "source": "### Linear regression example\n\nNow we do something (slightly) more interesting. We consider linear regression using PyTorch. We study the model\n\\begin{equation*}\n    y = \\beta + \\omega x + \\epsilon,\n\\end{equation*}\nwhere $\\epsilon$ represents the noise term.\n\nWe generate training data $\\{x_i, y_i\\}_{i=1}^I$ with $I = 100$ data points from the model with the true parameters $\\boldsymbol{\\phi} = \\begin{bmatrix} \\beta & \\omega\\end{bmatrix}^\\intercal = \\begin{bmatrix} 0.3 & 0.1 \\end{bmatrix}^\\intercal$ and normally distributed noise $\\epsilon \\sim \\mathcal{N}(0, 0.01^2)$. The inputs $x_i$ are sampled uniformly from the interval $[0,1]$.\n\nWe train the model on CPU by default."}, {"cell_type": "code", "execution_count": null, "id": "4ef12b4a", "metadata": {"tags": []}, "outputs": [], "source": "phi_true = torch.tensor([0.3, 0.1])\nn = 100\n\ntorch.manual_seed(1234)\nx = torch.rand(n)\ny = phi_true[0] + phi_true[1] * x + 0.01 * torch.randn(n)"}, {"cell_type": "markdown", "id": "cd16c205", "metadata": {"tags": []}, "source": "We plot the data to see what it looks like."}, {"cell_type": "code", "execution_count": null, "id": "9e807cd8", "metadata": {"tags": []}, "outputs": [], "source": "plt.scatter(x, y, label='data')\nplt.plot([0, 1], [phi_true[0], phi_true[0] + phi_true[1]], 'k--', label='true model')\nplt.legend()\nplt.show()"}, {"cell_type": "markdown", "id": "4a4df415", "metadata": {"tags": []}, "source": "We want to estimate the parameters $\\boldsymbol{\\phi} = \\begin{bmatrix} \\beta & \\omega\\end{bmatrix}^\\intercal$ of the linear regression model based on the training data $\\{x_i,y_i\\}_{i=1}^I$. In addition, we proceed by implementing the mean squared error cost function $L[\\boldsymbol{\\phi}]$, where\n\\begin{equation*}\nL[\\boldsymbol{\\phi}] = \\frac{1}{I} \\sum_{i=1}^I {(\\beta + \\omega x_i \u2212 y_i)}^2 = \\frac{1}{I} {\\|\\mathbf{X} \\boldsymbol{\\phi} \u2212 \\mathbf{y} \\|}^2_2.\n\\end{equation*}\nWe multiply the input $\\mathbf{X}$ with the variable $\\boldsymbol{\\phi}$, subtract the corresponding outputs, take the square, and finally average over all data samples.\n\nNote: starting from Python 3.5, many NumPy-style operators\u2014such as `@` for matrix multiplication and `**` for element-wise exponentiation\u2014are supported at the language level and can be directly applied to tensor matrices as well. "}, {"cell_type": "code", "execution_count": null, "id": "1c421b77", "metadata": {"tags": []}, "outputs": [], "source": "# define the initial model parameters as a column vector\nphi = torch.ones(2, 1, requires_grad = True)\n\nX = torch.stack([torch.ones(n), x], dim=1)\nY = y.view(-1, 1)\n\ntraining_loss = []\nfor i in range(1000):\n    # TODO -- compute the mean squared error (as loss) for the current parameters\n    # Replace this line!\n    loss = (X @ phi).mean()\n    # perform backpropagation\n    loss.backward()\n    training_loss.append(loss.item())\n    if i == 0 or (i + 1) % 100 == 0:\n        print(f\"Epoch {i + 1:04d}: training loss {training_loss[-1]: 9.6f}\")\n    \n    # perform a gradient descent step\n    with torch.no_grad():\n        phi -= 0.1 * phi.grad\n    # reset the gradient information\n    phi.grad.zero_()\n    \nplt.figure()\nplt.plot(np.arange(1, len(training_loss) + 1), training_loss, 'o', label='training loss')\nplt.yscale('log')\nplt.xlabel('number of iterations')\nplt.legend()\nplt.show()\n"}, {"cell_type": "code", "execution_count": null, "id": "931638fc", "metadata": {"tags": []}, "outputs": [], "source": "# we have to detach from the graph since otherwise the automatic conversion\n# to a numpy array performed by matplotlib will error\nphi_numpy = phi.detach().numpy()\nprint(phi_numpy)\nplt.scatter(x, y, label = 'data')\nplt.plot([0, 1], [phi_true[0], phi_true[0] + phi_true[1]], 'k--', label = 'true model')\nplt.plot([0, 1], [phi_numpy[0], phi_numpy[0] + phi_numpy[1]], 'r-', label = 'estimated model')\nplt.legend()\nplt.show()"}, {"cell_type": "markdown", "id": "f43cea05", "metadata": {"tags": []}, "source": "# Part II: Classification of hand written digits with PyTorch\n\nIn Hand-in Assignment 1, you implemented a neural network for MNIST classification using NumPy. In this task, you will re-implement the same model using the PyTorch API. By keeping all hyperparameters identical, you should expect to obtain the same results as in your NumPy-based implementation.\n\n### Step-1: Load and normalize the MNIST training and test datasets using `torchvision`.\n\nIn this step, we use the `torchvision` dataset and transform APIs to load and normalize MNIST, and the `DataLoader` to efficiently batch and iterate over the data during training and evaluation. Specifically, `torchvision.datasets.MNIST` provides a ready-to-use wrapper that automatically handles downloading, caching, and accessing the dataset, returning image\u2013label pairs in a PyTorch-friendly format."}, {"cell_type": "code", "execution_count": null, "id": "a59f32ed", "metadata": {"tags": []}, "outputs": [], "source": "import torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport time\n\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\nfrom torchvision.utils import make_grid\nfrom torchvision.transforms import v2 as T\n\n\nseed = 123\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# T.Compose takes a list of transformations and applies them in sequence\ntransform = T.Compose([\n    T.PILToTensor(),\n    T.ToDtype(torch.float32, scale=True), # scale=True scales values to the interval [0,1] if needed\n])\n# Notice the new keyword argument added at the end!\ntrainset = MNIST(\n    './data',\n    train=True,\n    download=True,\n    transform=transform)\n\ntestset = MNIST(\n    './data',\n    train=False,\n    download=True,\n    transform=transform)\n\n# extract a complete PyTorch dataset\ndef extract(dataset):\n    datasize = len(dataset)\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=datasize, shuffle=False)\n    return next(iter(dataloader))\n\n# extract all test images and labels into PyTorch tensors\n# the training data will be loaded in batches during training\ntest_X, test_Y = extract(testset)\n\n# Notice how image is now a tensor\nprint(test_X.shape)"}, {"cell_type": "markdown", "id": "72d2ff8b", "metadata": {"tags": []}, "source": "### Step-2: Implement a multi-layer neural network\n\nPyTorch provides the `torch.nn.Module` API to define arbitrary network architectures while tracking all learnable parameters automatically. **You should use this API to implement exactly the same fully-connected network as you designed for Assignment 1.**\n\nNote: The input data $X$ are grayscale images of $28\\times 28$ pixels. The first dimension will be the number of data points that are provided to the network. The input data is flattend into a matrix with $28 \\times 28 = 784$ columns using `X.view(-1, 784)`, where each colum represents one pixel."}, {"cell_type": "code", "execution_count": null, "id": "5ed79471", "metadata": {"tags": []}, "outputs": [], "source": "def weight_init(net_l):\n    # network initialization function\n    if not isinstance(net_l, list):\n        net_l = [net_l]\n    for net in net_l:\n        for m in net.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.kaiming_uniform_(m.weight)\n                m.bias.data.fill_(0.0)\n\nclass Net(nn.Module):\n    def __init__(self, activation=\"sigmoid\"):\n        super(Net, self).__init__()\n        U1 = 250 # number of unit for hidden layer 1\n        \n        # define your neural layers here\n        # Implement the same fully-connected network as you designed for Assignment 1!\n        # TODO -- Replace the lines below with your network setting!\n        self.fc1 = nn.Linear(784, U1)\n        self.fc2 = nn.Linear(U1, 10)\n        # define the activation function\n        if activation == \"sigmoid\":\n            self.activate = nn.Sigmoid()\n        elif activation == \"relu\":\n            self.activate = nn.ReLU()\n        \n        # network initialization\n        self.apply(weight_init)\n        \n    def forward(self, input):\n        # flatten the data into a matrix with 28 x 28 = 784 columns\n        x = input.view(-1, 784)\n        \n        # compute the hidden layers and predictions with\n        # e.g., self.fc1, self.fc2, self.fc3\n        # TODO -- Replace the lines below with your network setting!\n        x = self.activate(self.fc1(x))\n        x = self.fc2(x)\n        \n        return x\n"}, {"cell_type": "markdown", "id": "d1d10576", "metadata": {"tags": []}, "source": "**TODO:**  Compute how many parameters of this neural network?"}, {"cell_type": "markdown", "id": "a7c68c0e", "metadata": {"tags": []}, "source": "### Step-3: Loss function and predict function\n\nWe define the cross-entropy for the predicted probabilities $G$ (10-dimensional vectors) and the labels $Y$ (integers between 0 and 9). In addition, we use the `argmax` function to take final predictions.\n\nNote: In PyTorch, we use `torch.nn.functional.cross_entropy` which internally applies a softmax to the model logits, so the input should be raw, unnormalized scores rather than probabilities."}, {"cell_type": "code", "execution_count": null, "id": "f0200664", "metadata": {"tags": []}, "outputs": [], "source": "def cross_entropy(G, Y):\n    # TODO -- implement the cross entropy loss\n    # use F.cross_entropy\n    # Replace this line!\n    loss = G.mean()\n    return loss\n"}, {"cell_type": "code", "execution_count": null, "id": "604dcada", "metadata": {"tags": []}, "outputs": [], "source": "def predict(net, X):\n    return net(X).argmax(dim=1)\n\ndef accuracy(G, Y):\n    return (G.argmax(dim=1) == Y).float().mean()"}, {"cell_type": "markdown", "id": "63be9995", "metadata": {"tags": []}, "source": "### Step-4: Train the network\n\nYou should adopt the same settings e.g., batch size, the number of iterations, optimizer, learning rate, etc., as you did in Assignment-1 to train the network."}, {"cell_type": "code", "execution_count": null, "id": "90ede090", "metadata": {"tags": []}, "outputs": [], "source": "# initialize the test and training error statistics\ntest_accuracy = []\ntest_costs = []\ntest_iter = []\ntrain_accuracy = []\ntrain_costs = []\n\n# initialize the neural network and move it to the GPU if needed\nnet = Net(activation=\"relu\")\nnet.to(device)\n\n# define the optimization algorithm\nlearning_rate = 0.1\noptimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n\n# define the data loader for batches of the training data\nbatch_size = 100\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, num_workers=2, shuffle=True)\n\n# perform multiple training steps\nnum_epochs = 5\n\nt = 0 # current iteration\ndone = False\nstart_time = time.time()\nfor epoch in range(num_epochs):\n    for (batch_X, batch_Y) in trainloader:\n        # move batch to the GPU if needed\n        batch_X, batch_Y = batch_X.to(device), batch_Y.to(device)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # TODO -- Network forward, loss calculation, and backward!\n        # Replace the lines below!\n        batch_G = batch_Y\n        loss = (batch_G - batch_Y).mean()\n        # perform gradient descent step\n        optimizer.step()\n        \n        # don't bother too much about the following lines!\n        with torch.no_grad():\n            # evaluate the performance on the training data at every 10th iteration\n            if t % 100 == 0:\n                train_costs.append(loss.item())\n                train_accuracy.append(accuracy(batch_G, batch_Y).item())\n                \n            # evaluate the performance on the test data at every 100th iteration\n            if t % 100 == 0:\n                # move test data to the GPU if needed\n                X, Y = test_X.to(device), test_Y.to(device)\n\n                # compute predictions for the test data\n                G = net(X)\n                test_costs.append(cross_entropy(G, Y).item())\n                test_accuracy.append(accuracy(G, Y).item())\n                test_iter.append(t)\n\n                # print the iteration number and the accuracy of the predictions\n                print(f\"Epoch {epoch:2d} - Step {t:5d}: train accuracy {100 * train_accuracy[-1]:6.2f}% \" \\\n                      f\"train costs {train_costs[-1]:5.2f}  \" \\\n                      f\"test accuracy {100 * test_accuracy[-1]:6.2f}% \" \\\n                      f\"test costs {test_costs[-1]:5.2f}\")\n            \n        # accumulate iterations\n        t += 1\n            \nend_time = time.time()\nelapsed = end_time - start_time\n"}, {"cell_type": "markdown", "id": "aeb83015", "metadata": {"tags": []}, "source": "### Step-5: Evaluation on the test data"}, {"cell_type": "code", "execution_count": null, "id": "8bcd1d7e", "metadata": {"tags": []}, "outputs": [], "source": "# plot the cross-entropy\ndef training_curve_plot(title, \n                        train_costs, test_costs, train_accuracy, test_accuracy, \n                        batch_size, learning_rate, num_epochs, elapsed):\n    # Plot training curves in a format recomended for Hand-in assignment 1 and 2\n    #\n    # Input\n    # title - title for the plot\n    # train_costs - Array of training costs\n    # test_costs - Array of test costs\n    # train_accuracies - Array of training accuracies\n    # test_accuracies - Array of test accuracies\n    # batch_size - batch size for training data used during training\n    # num_epochs - Number of epochs used during training\n    # elapsed - Time elapsed in seconds during training\n\n    lg=18\n    md=13\n    sm=9\n    fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n    fig.suptitle(title, y=1.15, fontsize=lg)    \n    elapsed_min, elapsed_sec = divmod(elapsed, 60)\n    sub = f'|  Batch size:{batch_size}  |  Learning rate:{learning_rate} | Number of Epochs:{num_epochs} ' \\\n                + f'| Training time: {elapsed_min:.0f} min {elapsed_sec:.1f} sec |'\n    fig.text(0.5, 0.99, sub, ha='center', fontsize=md)\n    x = np.array(range(1, len(train_costs)+1))*num_epochs/len(train_costs)\n    axs[0].plot(x, train_costs, label=f'Final train cost: {train_costs[-1]:.4f}')\n    axs[0].plot(x, test_costs, label=f'Final test cost: {test_costs[-1]:.4f}')\n    axs[0].set_title('Costs', fontsize=md)\n    axs[0].set_xlabel('Epochs', fontsize=md)\n    axs[0].set_ylabel('Cost', fontsize=md)\n    axs[0].legend(fontsize=sm)\n    axs[0].tick_params(axis='both', labelsize=sm)\n    # Optionally use a logarithmic y-scale\n    #axs[0].set_yscale('log')\n    axs[1].plot(x, train_accuracy, label=f'Final train accuracy: {100*train_accuracy[-1]:.2f}%')\n    axs[1].plot(x, test_accuracy, label=f'Final test accuracy: {100*test_accuracy[-1]:.2f}%')\n    axs[1].set_title('Accuracy', fontsize=md)\n    axs[1].set_xlabel('Epochs', fontsize=md)\n    axs[1].set_ylabel('Accuracy (%)', fontsize=sm)\n    axs[1].legend(fontsize=sm)\n    axs[1].tick_params(axis='both', labelsize=sm)\n\ntitle = \"Training curve of the fully-connected network\"\ntraining_curve_plot(title, \n                    train_costs, test_costs, train_accuracy, test_accuracy, \n                    batch_size, learning_rate, num_epochs, elapsed)"}, {"cell_type": "code", "execution_count": null, "id": "0139e41e", "metadata": {"tags": []}, "outputs": [], "source": "# evaluate the network on 100 random test images\nwith torch.no_grad():\n    # obtain 100 random samples from the test data set\n    random_X, random_Y = next(iter(torch.utils.data.DataLoader(testset, batch_size=100, shuffle=True)))\n    \n    # move data to the GPU if needed\n    random_X, random_Y = random_X.to(device), random_Y.to(device)\n    \n    # compute the predictions for the sampled inputs\n    random_Yhat = predict(net, random_X)\n\n    # sort the predictions with the incorrect ones first\n    indices_incorrect_first = (random_Yhat == random_Y).float().argsort()\n\n# plot the images\nnum_rows = 10\nnum_cols = 10\nnum_images = num_rows * num_cols\nplt.figure(figsize=(num_cols, num_rows))\n\nfor i, index in enumerate(indices_incorrect_first, 1):\n    plt.subplot(num_rows, num_cols, i)\n    plt.xticks([])\n    plt.yticks([])\n    \n    # plot the image\n    plt.imshow(random_X[index, :, :].view(28, 28).cpu().numpy(), cmap=plt.cm.binary)\n    \n    # add the prediction as annotation (incorrect predictions in red, correct ones in blue)\n    color = 'blue' if random_Yhat[index] == random_Y[index] else 'red'\n    plt.text(0, 25, random_Yhat[index].item(), fontsize=25, color=color)\n        \nplt.show()"}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.19"}}, "nbformat": 4, "nbformat_minor": 5}